# Model Quantization with GPTQ AWQ and GGUF

## ðŸ”— Available Quantized Models and Notebooks

| Quantization Library | Description | Author | Colab Notebook |
|----------------------|-------------|----------------|----------------|
| llama.cpp (GGUF) | Quantize Llama 2 models using GGUF and llama.cpp, [blog](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | Maxime Labonne | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing) |
| ExLlamaV2 (GPTQ) | ExLlamaV2: The Fastest Library to Run LLMs by maximelabonne, [blog](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) |Maxime Labonne | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing) |
| AutoGPTQ (GPTQ)       | 4-bit LLM Quantization with GPTQ by maximelabonne, [blog](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html) | Maxime Labonne | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing) |


