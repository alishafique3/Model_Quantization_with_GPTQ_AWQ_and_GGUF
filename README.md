# Model Quantization with GPTQ AWQ and GGUF

## ðŸ”— Available Quantized Models and Notebooks

| Quantization Library | Description | Colab Notebook |
|----------------------|-------------|----------------|
| llama.cpp (GGUF) | Quantize Llama 2 models using GGUF and llama.cpp by maximelabonne | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing) |
| ExLlamaV2 (GPTQ) | ExLlamaV2: The Fastest Library to Run LLMs by maximelabonne | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing) |
| AutoGPTQ (GPTQ)       | 4-bit LLM Quantization with GPTQ by maximelabonne, [Blog](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing) |


