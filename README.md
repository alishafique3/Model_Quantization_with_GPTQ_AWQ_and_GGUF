# Model Quantization with GPTQ AWQ and GGUF

## ðŸ”— Available Quantized Models and Notebooks

| Quantization Library | Description | Colab Notebook |
|----------------------|-------------|----------------|
| llama.cpp (GGUF) | Quantize Llama 2 models using GGUF and llama.cpp by maximelabonne | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing) |
| llama.cpp (GGUF Q8_0) | 8-bit quantized model with better accuracy | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| AutoGPTQ (INT4)       | GPTQ quantized model for fast GPU inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| AWQ                  | Activation-aware quantization for latency-optimized inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |


