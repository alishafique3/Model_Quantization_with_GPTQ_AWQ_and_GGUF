# Model Quantization with GPTQ AWQ and GGUF
[In progress]

## ðŸ”— Available Models and Notebooks

| GGUF Model | Description | Colab Notebook |
|------------|-------------|----------------|
| `model-q4_0.gguf` | Quantized LLM model in Q4_0 format, suitable for CPU inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| `model-q8_0.gguf` | Higher precision GGUF model for better accuracy | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
