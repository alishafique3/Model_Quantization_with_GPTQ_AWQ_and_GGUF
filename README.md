# Model Quantization with GPTQ AWQ and GGUF

## ðŸ”— Available Quantized Models and Notebooks

| Quantization Library | Description | Colab Notebook |
|----------------------|-------------|----------------|
| llama.cpp (GGUF Q4_0) | 4-bit quantized model for efficient CPU inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| llama.cpp (GGUF Q8_0) | 8-bit quantized model with better accuracy | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| AutoGPTQ (INT4)       | GPTQ quantized model for fast GPU inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| AWQ                  | Activation-aware quantization for latency-optimized inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |


