# Model Quantization with GPTQ AWQ and GGUF

## ðŸ”— Available Quantized Models and Notebooks

| Quantization | Quantization Library | Description | Colab Notebook |
|--------------|----------------------|-------------|----------------|
| Q4_0         | GGUF (llama.cpp)     | 4-bit quantized model for efficient CPU inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| Q8_0         | GGUF (llama.cpp)     | 8-bit quantized model with higher accuracy | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| INT4         | AutoGPTQ             | GPTQ quantized for fast GPU inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |
| AWQ          | awq                  | Activation-aware weight quantization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/your-notebook-link-here) |

